{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/LLaMA.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chameleon in TransformerLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "from transformers import ChameleonForConditionalGeneration, AutoTokenizer, ChameleonProcessor\n",
    "# from transformers import ChameleonModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from jaxtyping import Float\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import HookedChameleon\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Chameleon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to load local chameleon model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"\"\n",
    "\n",
    "\n",
    "processor = ChameleonProcessor.from_pretrained(MODEL_PATH)\n",
    "hf_model = ChameleonForConditionalGeneration.from_pretrained(MODEL_PATH, low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedChameleon.from_pretrained(\n",
    "    \"\",\n",
    "    hf_model=hf_model,\n",
    "    device=\"cuda:2\",\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_and_idxs = list(zip(range(model.cfg.n_layers), model.blocks))\n",
    "for i, block in blocks_and_idxs:\n",
    "    print(f\"Block {i} is: {block}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.blocks[0].attn.norm_Q.weight)\n",
    "print(\"=\"*10)\n",
    "print(model.blocks[0].attn.norm_Q.bias)\n",
    "print(\"=\"*10)\n",
    "print(model.blocks[0].attn.norm_K.weight)\n",
    "print(\"=\"*10)\n",
    "print(model.blocks[0].attn.norm_K.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Where is the capital of Germany?\"\n",
    "input = processor(prompt, return_tensors=\"pt\")\n",
    "input_ids = input.input_ids\n",
    "print(input_ids)\n",
    "output = model.generate(input_ids, max_new_tokens=20, temperature=0)\n",
    "print(processor.tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Chameleon from transformers\n",
    "\n",
    "Load a chameleon model from transformers, and compare the outputs, the logits, and the hidden states to ensure we did a good job integrating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = hf_model.to(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Where is the capital of Germany?\"\n",
    "\n",
    "input = processor(prompt, return_tensors=\"pt\").to(\n",
    "            hf_model.device, dtype=hf_model.dtype\n",
    "        )\n",
    "print(input.input_ids)\n",
    "input_ids = input.input_ids\n",
    "\n",
    "output = hf_model.generate(input_ids.to(hf_model.device), max_new_tokens=20, do_sample=False)\n",
    "print(processor.tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_blocks_and_idxs = list(zip(range(hf_model.config.num_hidden_layers), hf_model.named_modules()))\n",
    "for i, block in hf_blocks_and_idxs:\n",
    "    print(f\"Block {i} is: {block}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare logits with HuggingFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Where is the capital of Germany?\",\n",
    "    \"Calculate 2 * 42 = \", \n",
    "    \"My favorite\", \n",
    "    \"My favorite place is\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "hf_model.eval()\n",
    "tokenizer = processor.tokenizer\n",
    "prompt_ids = [tokenizer.encode(prompt, return_tensors=\"pt\") for prompt in prompts]\n",
    "tl_logits = [model(prompt_ids).detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "\n",
    "logits = [hf_model(prompt_ids.to(hf_model.device)).logits.detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "\n",
    "for i in range(len(prompts)): \n",
    "    if not torch.allclose(logits[i], tl_logits[i], atol=1e-2, rtol=1e-2):\n",
    "        print(f\"Logits for prompt {i} are not close\")\n",
    "        print(f\"Logits from HuggingFace: shape {logits[i].shape}\")\n",
    "        print(f\"Logits from TransformerLens: shape {tl_logits[i].shape}\")\n",
    "        diff = torch.abs(logits[i] - tl_logits[i]) > 1e-2\n",
    "        indices = torch.nonzero(diff)\n",
    "        for index in indices:\n",
    "            row, col, loc = index[0], index[1], index[2]\n",
    "            print(f\"Diff at {index}: HuggingFace={logits[i][row, col, loc]}, TransformerLens={tl_logits[i][row, col, loc]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tl_hidden_states = [model(prompt_ids, return_type=\"hidden_states\", stop_at_layer=1).detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "hf_hidden_states = [hf_model(prompt_ids.to(hf_model.device), output_hidden_states=True, output_attentions=True).hidden_states[1].detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "\n",
    "for i in range(len(prompts)): \n",
    "    print(f\"Shape of hf hidden states: {hf_hidden_states[i].shape}\")\n",
    "    print(f\"Shape of tl hidden states: {tl_hidden_states[i].shape}\")\n",
    "    if not torch.allclose(hf_hidden_states[i], tl_hidden_states[i], atol=1e-4, rtol=1e-2):\n",
    "        print(f\"Hidden states for prompt {i} are not close\")\n",
    "    print(f\"Hidden states from HuggingFace: {hf_hidden_states[i]}\")\n",
    "    print(f\"Hidden states from TransformerLens: {tl_hidden_states[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare attentions\n",
    "\n",
    "tl_attentions = [model(prompt_ids, return_type=\"attentions\")[2].detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "hf_attentions = [hf_model(prompt_ids.to(hf_model.device), output_hidden_states=True, output_attentions=True).attentions[2].detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "\n",
    "for i in range(len(prompts)): \n",
    "    print(f\"Shape of hf attentions: {hf_attentions[i].shape}\")\n",
    "    print(f\"Shape of tl attentions: {tl_attentions[i].shape}\")\n",
    "    if not torch.allclose(hf_attentions[i], tl_attentions[i], atol=1e-4, rtol=1e-2):\n",
    "        print(f\"Attentions for prompt {i} are not close\")\n",
    "        print(f\"Attentions from HuggingFace: {hf_attentions[i]}\")\n",
    "        print(f\"Attentions from TransformerLens: {tl_attentions[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
