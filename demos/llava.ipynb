{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/LLaMA.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLAVA in TransformerLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    LlavaNextForConditionalGeneration,\n",
    "    LlavaNextProcessor,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from jaxtyping import Float\n",
    "\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.HookedLlava import HookedLlava\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading LLAVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to load local chameleon model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(MODEL_PATH)\n",
    "vision_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        MODEL_PATH, \n",
    "        torch_dtype=torch.float32, \n",
    "        low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "hf_model=vision_model.language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedLlava.from_pretrained(\n",
    "    MODEL_PATH, \n",
    "    hf_model=hf_model,\n",
    "    torch_dtype=torch.float32, \n",
    "    low_cpu_mem_usage=True,\n",
    "    device=\"cuda:2\",\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    tokenizer=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_and_idxs = list(zip(range(model.cfg.n_layers), model.blocks))\n",
    "for i, block in blocks_and_idxs:\n",
    "    print(f\"Block {i} is: {block}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_blocks_and_idxs = list(zip(range(hf_model.config.num_hidden_layers), hf_model.model.layers))\n",
    "\n",
    "for i, block in hf_blocks_and_idxs:\n",
    "    print(f\"Block {i} is: {block}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_params = model.state_dict()\n",
    "hf_block_params = hf_model.state_dict()\n",
    "print(block_params.keys())\n",
    "print(hf_block_params.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import einops\n",
    "for i in range(model.cfg.n_layers):\n",
    "    W_Q=einops.rearrange(block_params[f\"blocks.{i}.attn.W_Q\"], \"n m h -> (n h) m\")\n",
    "    W_K=einops.rearrange(block_params[f\"blocks.{i}.attn.W_K\"], \"n m h -> (n h) m\")\n",
    "    W_V=einops.rearrange(block_params[f\"blocks.{i}.attn.W_V\"], \"n m h -> (n h) m\")\n",
    "    W_O=einops.rearrange(block_params[f\"blocks.{i}.attn.W_O\"], \"n h m -> m (n h)\")\n",
    "    \n",
    "    device = \"cuda:2\"\n",
    "    if not torch.equal(W_Q.to(device),hf_block_params[f\"model.layers.{i}.self_attn.q_proj.weight\"].to(device)):\n",
    "        print(f\"Block {i} W_Q does not match\")\n",
    "    if not torch.equal(W_K.to(device),hf_block_params[f\"model.layers.{i}.self_attn.k_proj.weight\"].to(device)):\n",
    "        print(f\"Block {i} W_K does not match\")\n",
    "    if not torch.equal(W_V.to(device),hf_block_params[f\"model.layers.{i}.self_attn.v_proj.weight\"].to(device)):\n",
    "        print(f\"Block {i} W_V does not match\")\n",
    "    if not torch.equal(W_O.to(device),hf_block_params[f\"model.layers.{i}.self_attn.o_proj.weight\"].to(device)):\n",
    "        print(f\"Block {i} W_O does not match\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Where is the capital of Germany?\"\n",
    "input = processor(prompt, return_tensors=\"pt\")\n",
    "input_ids = input.input_ids\n",
    "print(input_ids)\n",
    "output = model.generate(input_ids, max_new_tokens=20, temperature=0)\n",
    "print(processor.tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.blocks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "        \"The capital of Germany is\",\n",
    "        \"2 * 42 = \", \n",
    "        \"My favorite\", \n",
    "        \"aosetuhaosuh aostud aoestuaoentsudhasuh aos tasat naostutshaosuhtnaoe usaho uaotsnhuaosntuhaosntu haouaoshat u saotheu saonuh aoesntuhaosut aosu thaosu thaoustaho usaothusaothuao sutao sutaotduaoetudet uaosthuao uaostuaoeu aostouhsaonh aosnthuaoscnuhaoshkbaoesnit haosuhaoe uasotehusntaosn.p.uo ksoentudhao ustahoeuaso usant.hsa otuhaotsi aostuhs\",\n",
    "    ]\n",
    "    \n",
    "model.eval()\n",
    "hf_model.eval()\n",
    "tokenizer=AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model_device =\"cuda:0\"\n",
    "hf_model_device = \"cuda:1\"\n",
    "model=model.to(model_device)\n",
    "hf_model=hf_model.to(hf_model_device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "prompt_id_tl = tokenizer.encode(prompt, return_tensors=\"pt\").to(model_device)  \n",
    "prompt_id_hf = tokenizer.encode(prompt, return_tensors=\"pt\").to(hf_model_device)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hook_fn(module_name, module, input, output):\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]  \n",
    "    return {module_name: output.detach().cpu()}\n",
    "\n",
    "tl_internal_outputs = {}\n",
    "hf_internal_outputs = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def register_hf_hooks(hf_model):\n",
    "    hf_model.model.layers[0].input_layernorm.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"input_layernorm\", m, i, o)))\n",
    "    hf_model.model.layers[0].self_attn.q_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"self_attn.q_proj\", m, i, o)))\n",
    "    hf_model.model.layers[0].self_attn.o_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"self_attn.o_proj\", m, i, o)))\n",
    "    hf_model.model.layers[0].mlp.gate_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"mlp.gate_proj\", m, i, o)))\n",
    "    hf_model.model.layers[0].mlp.down_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"mlp.down_proj\", m, i, o)))\n",
    "\n",
    "def register_tl_hooks(model):\n",
    "    model.blocks[0].hook_resid_pre.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_resid_pre\", m, i, o)))\n",
    "    model.blocks[0].attn.hook_q.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_attn_in\", m, i, o)))\n",
    "    model.blocks[0].attn.hook_z.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_attn_out\", m, i, o)))\n",
    "    model.blocks[0].mlp.hook_pre.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_mlp_in\", m, i, o)))\n",
    "    model.blocks[0].mlp.hook_post.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_mlp_out\", m, i, o)))\n",
    "\n",
    "register_hf_hooks(hf_model)\n",
    "\n",
    "register_tl_hooks(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hf_model.device)\n",
    "print(prompt_id_hf.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_logits = model(prompt_id_tl).detach().cpu()\n",
    "hf_logits = hf_model(prompt_id_hf).logits.detach().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_mapping = {\n",
    "    \"hook_attn_in\": \"self_attn.q_proj\",\n",
    "    \"hook_attn_out\": \"self_attn.o_proj\",\n",
    "    \"hook_mlp_in\": \"mlp.gate_proj\",\n",
    "    \"hook_mlp_out\": \"mlp.down_proj\"\n",
    "}\n",
    "\n",
    "for tl_key, hf_key in module_mapping.items():\n",
    "    if tl_key in tl_internal_outputs and hf_key in hf_internal_outputs:\n",
    "        tl_value = tl_internal_outputs[tl_key]\n",
    "        hf_value = hf_internal_outputs[hf_key]\n",
    "        print(tl_value.shape)\n",
    "        \n",
    "        print(hf_value.shape)\n",
    "        if tl_key==\"hook_attn_in\" or tl_key==\"hook_attn_out\":\n",
    "            tl_value=tl_value.reshape(1,8,4096)\n",
    "\n",
    "        if not torch.allclose(tl_value, hf_value, atol=1e-4, rtol=1e-2):\n",
    "            print(f\"Difference found in {tl_key} (TL) vs {hf_key} (HF):\")\n",
    "            print(f\"HookedTransformer output: {tl_value}\")\n",
    "            print(f\"Hugging Face output: {hf_value}\")\n",
    "            print(f\"Difference: {tl_value - hf_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_fn(module_name, module, input, output):\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]  \n",
    "    return {module_name: output.detach().cpu()}\n",
    "\n",
    "tl_internal_outputs = {}\n",
    "hf_internal_outputs = {}\n",
    "\n",
    "module_mapping = {\n",
    "    \"hook_resid_pre\": \"input_layernorm\",\n",
    "    \"hook_attn_in\": \"self_attn.q_proj\",\n",
    "    \"hook_attn_out\": \"self_attn.o_proj\",\n",
    "    \"hook_mlp_in\": \"mlp.gate_proj\",\n",
    "    \"hook_mlp_out\": \"mlp.down_proj\"\n",
    "}\n",
    "\n",
    "def register_hf_hooks(hf_model):\n",
    "    hf_model.model.layers[0].input_layernorm.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"input_layernorm\", m, i, o)))\n",
    "    hf_model.model.layers[0].self_attn.q_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"self_attn.q_proj\", m, i, o)))\n",
    "    hf_model.model.layers[0].self_attn.o_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"self_attn.o_proj\", m, i, o)))\n",
    "    hf_model.model.layers[0].mlp.gate_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"mlp.gate_proj\", m, i, o)))\n",
    "    hf_model.model.layers[0].mlp.down_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"mlp.down_proj\", m, i, o)))\n",
    "\n",
    "def register_tl_hooks(model):\n",
    "    model.blocks[0].hook_resid_pre.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_resid_pre\", m, i, o)))\n",
    "    model.blocks[0].attn.hook_q.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_attn_in\", m, i, o)))\n",
    "    model.blocks[0].attn.hook_z.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_attn_out\", m, i, o)))\n",
    "    model.blocks[0].mlp.hook_pre.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_mlp_in\", m, i, o)))\n",
    "    model.blocks[0].mlp.hook_post.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_mlp_out\", m, i, o)))\n",
    "\n",
    "register_hf_hooks(hf_model)\n",
    "\n",
    "register_tl_hooks(model)\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "prompt_id_tl = tokenizer.encode(prompt, return_tensors=\"pt\").to(model_device)  \n",
    "prompt_id_hf = tokenizer.encode(prompt, return_tensors=\"pt\").to(hf_model_device)  \n",
    "tl_logits = model(prompt_id_tl).detach().cpu()\n",
    "hf_logits = hf_model(prompt_id_hf).logits.detach().cpu()\n",
    "\n",
    "for tl_key, hf_key in module_mapping.items():\n",
    "    if tl_key in tl_internal_outputs and hf_key in hf_internal_outputs:\n",
    "        tl_value = tl_internal_outputs[tl_key]\n",
    "        hf_value = hf_internal_outputs[hf_key]\n",
    "        if not torch.allclose(tl_value, hf_value, atol=1e-4, rtol=1e-2):\n",
    "            print(f\"Difference found in {tl_key} (TL) vs {hf_key} (HF):\")\n",
    "            print(f\"HookedTransformer output: {tl_value}\")\n",
    "            print(f\"Hugging Face output: {hf_value}\")\n",
    "            print(f\"Difference: {tl_value - hf_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Processing prompt {i+1}/{len(prompts)}\")\n",
    "\n",
    "    prompt_id = tokenizer.encode(prompt, return_tensors=\"pt\").to(model_device)\n",
    "    prompt_id_hf = tokenizer.encode(prompt, return_tensors=\"pt\").to(hf_model_device)\n",
    "\n",
    "    tl_input = prompt_id\n",
    "    hf_input = prompt_id_hf\n",
    "    \n",
    "    tl_layer_output = model.blocks[0](tl_input)\n",
    "    hf_layer_output = hf_model.model.layers[0](hf_input)\n",
    "\n",
    "    if not torch.allclose(hf_layer_output, tl_layer_output, atol=1e-4, rtol=1e-2):\n",
    "        print(f\"Difference found at layer 0 for prompt {i}:\")\n",
    "        print(f\"hf_layer_output: {hf_layer_output}\")\n",
    "        print(f\"tl_layer_output: {tl_layer_output}\")\n",
    "        print(f\"Difference: {hf_layer_output - tl_layer_output}\")\n",
    "\n",
    "        abs_diff = torch.max(torch.abs(hf_layer_output - tl_layer_output))\n",
    "        rel_diff = torch.max(torch.abs((hf_layer_output - tl_layer_output) / (tl_layer_output + 1e-8)))\n",
    "        print(f\"Max absolute difference at layer 0: {abs_diff.item()}\")\n",
    "        print(f\"Max relative difference at layer 0: {rel_diff.item()}\")\n",
    "\n",
    "        if not torch.allclose(hf_layer_output, tl_layer_output, atol=1e-3, rtol=1e-2):\n",
    "            print(f\"Larger difference persists at layer 0 for prompt {i}, investigate further.\")\n",
    "\n",
    "    assert torch.allclose(hf_layer_output, tl_layer_output, atol=1e-4, rtol=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading LLAVA from transformers\n",
    "\n",
    "Load a chameleon model from transformers, and compare the outputs, the logits, and the hidden states to ensure we did a good job integrating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = hf_model.to(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Where is the capital of Germany?\"\n",
    "input = processor(prompt, return_tensors=\"pt\").to(\n",
    "            hf_model.device, dtype=hf_model.dtype\n",
    "        )\n",
    "print(input.input_ids)\n",
    "input_ids = input.input_ids\n",
    "\n",
    "output = hf_model.generate(input_ids.to(hf_model.device), max_new_tokens=20, do_sample=False)\n",
    "print(processor.tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get shape of the weights\n",
    "# for name in hf_model.state_dict():\n",
    "#     print(name, hf_model.state_dict()[name].shape)\n",
    "    \n",
    "# print(hf_model.state_dict()[\"model.layers.0.self_attn.q_norm.weight\"])\n",
    "# print(hf_model.state_dict()[\"model.layers.0.input_layernorm.weight\"])\n",
    "# print(hf_model.state_dict()[\"model.layers.0.post_attention_layernorm.weight\"])\n",
    "# print(hf_model.state_dict()[\"model.layers.0.self_attn.q_norm.weight\"])\n",
    "# print(hf_model.state_dict()[\"model.layers.0.self_attn.q_proj.weight\"])\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_blocks_and_idxs = list(zip(range(hf_model.config.num_hidden_layers), hf_model.named_modules()))\n",
    "for i, block in hf_blocks_and_idxs:\n",
    "    print(f\"Block {i} is: {block}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare logits with HuggingFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Where is the capital of Germany?\",\n",
    "    \"Calculate 2 * 42 = \", \n",
    "    \"My favorite\", \n",
    "    \"My favorite place is\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "hf_model.eval()\n",
    "tokenizer = processor.tokenizer\n",
    "prompt_ids = [tokenizer.encode(prompt, return_tensors=\"pt\") for prompt in prompts]\n",
    "tl_logits = [model(prompt_ids).detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "\n",
    "# hf logits are really slow as it's on CPU. If you have a big/multi-GPU machine, run `hf_model = hf_model.to(\"cuda\")` to speed this up\n",
    "logits = [hf_model(prompt_ids.to(hf_model.device)).logits.detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "\n",
    "for i in range(len(prompts)): \n",
    "    if not torch.allclose(logits[i], tl_logits[i], atol=1e-2, rtol=1e-2):\n",
    "        print(f\"Logits for prompt {i} are not close\")\n",
    "        print(f\"Logits from HuggingFace: shape {logits[i].shape}\")\n",
    "        print(f\"Logits from TransformerLens: shape {tl_logits[i].shape}\")\n",
    "        diff = torch.abs(logits[i] - tl_logits[i]) > 1e-2\n",
    "        indices = torch.nonzero(diff)\n",
    "        for index in indices:\n",
    "            row, col, loc = index[0], index[1], index[2]\n",
    "            print(f\"Diff at {index}: HuggingFace={logits[i][row, col, loc]}, TransformerLens={tl_logits[i][row, col, loc]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare hidden states\n",
    "\n",
    "tl_hidden_states = [model(prompt_ids, return_type=\"hidden_states\", stop_at_layer=1).detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "hf_hidden_states = [hf_model(prompt_ids.to(hf_model.device), output_hidden_states=True, output_attentions=True).hidden_states[1].detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "\n",
    "for i in range(len(prompts)): \n",
    "    print(f\"Shape of hf hidden states: {hf_hidden_states[i].shape}\")\n",
    "    print(f\"Shape of tl hidden states: {tl_hidden_states[i].shape}\")\n",
    "    if not torch.allclose(hf_hidden_states[i], tl_hidden_states[i], atol=1e-4, rtol=1e-2):\n",
    "        print(f\"Hidden states for prompt {i} are not close\")\n",
    "    print(f\"Hidden states from HuggingFace: {hf_hidden_states[i]}\")\n",
    "    print(f\"Hidden states from TransformerLens: {tl_hidden_states[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare attentions\n",
    "\n",
    "tl_attentions = [model(prompt_ids, return_type=\"attentions\")[2].detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "hf_attentions = [hf_model(prompt_ids.to(hf_model.device), output_hidden_states=True, output_attentions=True).attentions[2].detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "\n",
    "for i in range(len(prompts)): \n",
    "    print(f\"Shape of hf attentions: {hf_attentions[i].shape}\")\n",
    "    print(f\"Shape of tl attentions: {tl_attentions[i].shape}\")\n",
    "    if not torch.allclose(hf_attentions[i], tl_attentions[i], atol=1e-4, rtol=1e-2):\n",
    "        print(f\"Attentions for prompt {i} are not close\")\n",
    "        print(f\"Attentions from HuggingFace: {hf_attentions[i]}\")\n",
    "        print(f\"Attentions from TransformerLens: {tl_attentions[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerLens Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_text = \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.\"\n",
    "llama_tokens = model.to_tokens(llama_text)\n",
    "llama_logits, llama_cache = model.run_with_cache(llama_tokens, remove_batch_dim=True)\n",
    "\n",
    "attention_pattern = llama_cache[\"pattern\", 0, \"attn\"]\n",
    "llama_str_tokens = model.to_str_tokens(llama_text)\n",
    "\n",
    "print(\"Layer 0 Head Attention Patterns:\")\n",
    "display(cv.attention.attention_patterns(tokens=llama_str_tokens, attention=attention_pattern))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_to_ablate = 0\n",
    "head_index_to_ablate = 31\n",
    "\n",
    "# We define a head ablation hook\n",
    "# The type annotations are NOT necessary, they're just a useful guide to the reader\n",
    "# \n",
    "def head_ablation_hook(\n",
    "    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
    "    hook: HookPoint\n",
    ") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n",
    "    print(f\"Shape of the value tensor: {value.shape}\")\n",
    "    value[:, :, head_index_to_ablate, :] = 0.\n",
    "    return value\n",
    "\n",
    "original_loss = model(llama_tokens, return_type=\"loss\")\n",
    "ablated_loss = model.run_with_hooks(\n",
    "    llama_tokens, \n",
    "    return_type=\"loss\", \n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"v\", layer_to_ablate), \n",
    "        head_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "print(f\"Original Loss: {original_loss.item():.3f}\")\n",
    "print(f\"Ablated Loss: {ablated_loss.item():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('tl-llama': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f03ec946e3b5caa7cc710a963f479e62a68fff56c790a7066e03c8b5c22adad9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
